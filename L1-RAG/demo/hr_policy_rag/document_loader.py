"""
æ–‡æ¡£åŠ è½½æ¨¡å—ï¼šè´Ÿè´£åŠ è½½å’Œé¢„å¤„ç†HRåˆ¶åº¦æ–‡æ¡£

æ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼š
1. æ–‡æ¡£åŠ è½½ï¼šæ”¯æŒå¤šç§æ ¼å¼ï¼ˆTXTã€PDFã€Wordç­‰ï¼‰
2. æ–‡æ¡£åˆ†å—ï¼šå°†é•¿æ–‡æ¡£åˆ‡åˆ†ä¸ºé€‚åˆå‘é‡åŒ–çš„æ–‡æœ¬å—
3. åˆ†å—ç­–ç•¥ï¼šå›ºå®šçª—å£ã€æ»‘åŠ¨çª—å£ã€æŒ‰æ®µè½åˆ†å—
"""
import os
from typing import List
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain.schema import Document


class DocumentLoader:
    """æ–‡æ¡£åŠ è½½å™¨ï¼šè´Ÿè´£åŠ è½½å’Œåˆ†å—æ–‡æ¡£"""
    
    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 75):
        """
        åˆå§‹åŒ–æ–‡æ¡£åŠ è½½å™¨
        
        Args:
            chunk_size: æ¯ä¸ªæ–‡æœ¬å—çš„å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            chunk_overlap: æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼ˆç”¨äºä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§ï¼‰
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨
        # RecursiveCharacterTextSplitterä¼šæ™ºèƒ½åœ°æŒ‰ç…§åˆ†éš”ç¬¦ä¼˜å…ˆçº§è¿›è¡Œåˆ†å‰²
        # ä¼˜å…ˆæŒ‰æ®µè½åˆ†å‰²ï¼Œç„¶åæ˜¯å¥å­ï¼Œæœ€åæ˜¯å­—ç¬¦
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n## ", "\n### ", "\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""]  # åˆ†éš”ç¬¦ä¼˜å…ˆçº§
        )
    
    def load_text_file(self, file_path: str) -> List[Document]:
        """
        åŠ è½½æ–‡æœ¬æ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            Documentåˆ—è¡¨
        """
        loader = TextLoader(file_path, encoding='utf-8')
        documents = loader.load()
        return documents
    
    def load_pdf_file(self, file_path: str) -> List[Document]:
        """
        åŠ è½½PDFæ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            Documentåˆ—è¡¨
        """
        loader = PyPDFLoader(file_path)
        documents = loader.load()
        return documents
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        å°†æ–‡æ¡£åˆ†å‰²æˆæ–‡æœ¬å—
        
        Args:
            documents: åŸå§‹æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            åˆ†å‰²åçš„æ–‡æ¡£å—åˆ—è¡¨
        """
        # ä½¿ç”¨RecursiveCharacterTextSplitterè¿›è¡Œæ™ºèƒ½åˆ†å‰²
        chunks = self.text_splitter.split_documents(documents)
        
        # ä¸ºæ¯ä¸ªå—æ·»åŠ å…ƒæ•°æ®ï¼ˆæ¥æºä¿¡æ¯ï¼‰
        for i, chunk in enumerate(chunks):
            if not chunk.metadata.get('source'):
                chunk.metadata['source'] = 'unknown'
            chunk.metadata['chunk_id'] = i
        
        return chunks
    
    def load_and_split(self, file_path: str) -> List[Document]:
        """
        åŠ è½½æ–‡ä»¶å¹¶è‡ªåŠ¨åˆ†å‰²ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            åˆ†å‰²åçš„æ–‡æ¡£å—åˆ—è¡¨
        """
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©åŠ è½½å™¨
        file_ext = os.path.splitext(file_path)[1].lower()
        
        if file_ext == '.txt':
            documents = self.load_text_file(file_path)
        elif file_ext == '.pdf':
            documents = self.load_pdf_file(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
        
        # åˆ†å‰²æ–‡æ¡£
        chunks = self.split_documents(documents)
        
        print(f"âœ… æˆåŠŸåŠ è½½æ–‡æ¡£: {file_path}")
        print(f"ğŸ“„ åŸå§‹æ–‡æ¡£æ•°: {len(documents)}")
        print(f"ğŸ“¦ åˆ†å‰²åæ–‡æœ¬å—æ•°: {len(chunks)}")
        print(f"ğŸ“Š å¹³å‡æ¯ä¸ªæ–‡æœ¬å—å¤§å°: {sum(len(chunk.page_content) for chunk in chunks) // len(chunks)} å­—ç¬¦")
        
        return chunks


def demo_document_loading():
    """æ¼”ç¤ºæ–‡æ¡£åŠ è½½åŠŸèƒ½"""
    print("=" * 60)
    print("æ–‡æ¡£åŠ è½½æ¨¡å—æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºæ–‡æ¡£åŠ è½½å™¨
    loader = DocumentLoader(chunk_size=500, chunk_overlap=75)
    
    # åŠ è½½ç¤ºä¾‹æ–‡æ¡£
    data_dir = os.path.join(os.path.dirname(__file__), "data")
    file_path = os.path.join(data_dir, "hr_policy.txt")
    
    if os.path.exists(file_path):
        chunks = loader.load_and_split(file_path)
        
        # æ˜¾ç¤ºå‰3ä¸ªæ–‡æœ¬å—
        print("\nå‰3ä¸ªæ–‡æœ¬å—ç¤ºä¾‹ï¼š")
        for i, chunk in enumerate(chunks[:3], 1):
            print(f"\n--- æ–‡æœ¬å— {i} ---")
            print(f"å†…å®¹é•¿åº¦: {len(chunk.page_content)} å­—ç¬¦")
            print(f"æ¥æº: {chunk.metadata.get('source', 'unknown')}")
            print(f"å†…å®¹é¢„è§ˆ: {chunk.page_content[:200]}...")
    else:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")


if __name__ == "__main__":
    demo_document_loading()

