# HR制度智能问答系统 - RAG实战项目

## 📚 项目简介

这是一个基于**向量检索的RAG（检索增强生成）**实现的公司HR制度问答系统。通过这个项目，你将完整理解RAG的核心概念、技术实现和最佳实践。

### 🎯 学习目标

完成这个项目后，你将能够：

1. **理解RAG的核心原理**：检索(Retrieval) + 生成(Generation)的协同工作模式
2. **掌握文档处理技术**：文档加载、分块策略、预处理
3. **理解向量化技术**：Embedding模型、向量相似度计算
4. **掌握向量数据库**：FAISS的使用、向量存储和检索
5. **实现完整的RAG流程**：从文档到问答的端到端实现
6. **理解Prompt工程**：如何设计有效的提示词模板

## 🏗️ 项目结构

```
hr_policy_rag/
├── data/                    # HR制度文档数据
│   └── hr_policy.txt       # 示例HR制度文档
├── storage/                 # 向量存储目录（自动生成）
│   └── vectorstore/        # FAISS向量数据库文件
├── notebooks/              # Jupyter Notebook（可选）
├── config.py               # 配置文件
├── document_loader.py      # 文档加载和分块模块
├── vector_store.py         # 向量化和存储模块
├── rag_chain.py            # RAG链实现
├── main.py                 # 主程序入口
├── requirements.txt        # 项目依赖
└── README.md              # 本文件
```

## 🚀 快速开始

### 1. 环境准备

```bash
# 创建虚拟环境（推荐）
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 安装依赖
pip install -r requirements.txt
```

### 2. 配置API密钥

创建 `.env` 文件（在项目根目录）：

```bash
OPENAI_API_KEY=your_openai_api_key_here
```

或者直接设置环境变量：

```bash
export OPENAI_API_KEY=your_openai_api_key_here
```

### 3. 运行项目

```bash
# 运行主程序
python main.py
```

程序会自动：
1. 检查是否存在向量存储，如果不存在则构建
2. 加载知识库
3. 启动交互式问答界面

### 4. 使用示例

```
❓ 请输入您的问题: 年假如何申请？需要提前几天？

🔍 正在检索相关文档...
🤖 正在生成回答...

📝 回答:
根据《公司HR制度手册》第三章第3.1节规定，年假申请需要提前7个工作日申请，并经部门经理批准。具体流程如下：
1. 提前7个工作日提交年假申请
2. 经部门经理批准
3. 在OA系统中完成申请流程

📚 参考文档 (3 条):
  1. [hr_policy.txt] 年假需提前7个工作日申请，经部门经理批准...
  2. [hr_policy.txt] 入职满1年：5天年假...
  3. [hr_policy.txt] 年假不可跨年累计...
```

## 📖 核心知识点详解

### 1. RAG核心流程

```
用户提问 → 问题向量化 → 知识库检索(Top-K) → Prompt上下文增强 → LLM生成 → 带来源的回答
```

**关键优势**：
- ✅ **精准性**：基于最新知识片段，而非模型训练数据
- ✅ **透明度**：可追溯答案来源
- ✅ **安全性**：敏感数据本地存储，不上传到模型

### 2. 文档加载与分块 (`document_loader.py`)

#### 为什么需要分块？

- LLM有上下文长度限制（如GPT-3.5-turbo是4096 tokens）
- 长文档直接向量化会丢失细节信息
- 分块后可以精确检索到相关片段

#### 分块策略

```python
# 固定窗口分块：按字符数切割
chunk_size = 500  # 每个块500字符
chunk_overlap = 75  # 重叠75字符（保持上下文连续性）
```

**分块原则**：
- 技术文档：按API端点切分（300-500字）
- 法律条文：按条款切分
- 客服对话：按完整Q&A对切分

### 3. 向量化与存储 (`vector_store.py`)

#### Embedding向量化

将文本转换为高维向量（如1536维），使语义相似的句子在向量空间中距离更近。

**示例**：
- "年假如何申请" → [0.123, -0.456, 0.789, ...]
- "请假流程是什么" → [0.125, -0.451, 0.785, ...]  （相似度高）

#### 相似度计算

- **余弦相似度**：最常用，忽略向量长度差异，适合长文本
- **欧氏距离**：关注绝对距离，适合短文本精确匹配
- **内积（Dot Product）**：计算速度快，需向量归一化

#### FAISS向量数据库

- **优势**：开源、高效、支持CPU/GPU加速
- **适用场景**：中小规模知识库（<1M条）
- **工作原理**：建立索引，支持毫秒级检索

### 4. RAG链实现 (`rag_chain.py`)

#### Prompt模板设计

```python
prompt_template = """你是一位专业的HR助手，负责回答员工关于公司HR制度的问题。

请严格基于以下提供的公司制度文档内容回答问题。

公司制度文档内容：
{context}

问题: {question}
"""
```

**设计要点**：
1. 明确角色定位（HR助手）
2. 强调基于文档回答（减少幻觉）
3. 要求标注来源（提高可追溯性）
4. 控制回答格式（简洁、友好）

#### RAG流程

```python
# 1. 检索相关文档
retrieved_docs = vector_store.similarity_search(query, k=3)

# 2. 格式化文档为上下文
context = format_docs(retrieved_docs)

# 3. 注入到Prompt中
prompt = prompt_template.format(context=context, question=query)

# 4. 调用LLM生成回答
answer = llm.invoke(prompt)
```

## 🔧 模块详解

### `config.py` - 配置管理

集中管理所有配置项：
- API密钥
- 模型参数
- 分块参数
- 检索参数

### `document_loader.py` - 文档加载

**核心功能**：
- 支持多种格式（TXT、PDF、Word）
- 智能文档分块
- 元数据管理（来源、ID等）

**关键类**：
- `DocumentLoader`: 文档加载器

### `vector_store.py` - 向量存储

**核心功能**：
- 文档向量化
- 向量存储管理
- 相似度搜索

**关键类**：
- `VectorStoreManager`: 向量存储管理器

### `rag_chain.py` - RAG链

**核心功能**：
- 检索相关文档
- Prompt构建
- LLM调用
- 结果格式化

**关键类**：
- `RAGChain`: RAG链实现

### `main.py` - 主程序

**核心功能**：
- 知识库构建/加载
- 交互式问答界面
- 错误处理

## 📊 测试与验证

### 测试问题集

建议测试以下类型的问题：

1. **简单查询**：年假有多少天？
2. **流程查询**：年假如何申请？需要提前几天？
3. **复合查询**：产假有多少天？工资怎么发？
4. **边界情况**：文档中没有的信息

### 评估指标

- **检索准确性**：返回的文档是否相关？
- **生成准确性**：回答是否正确？
- **来源标注**：是否标注了信息来源？
- **响应时间**：端到端延迟是否可接受？

## 🎓 进阶学习

### 1. 优化检索策略

- **混合检索**：结合关键词检索（BM25）和向量检索
- **重排序（Re-ranking）**：使用更强大的模型对检索结果重新排序
- **查询扩展**：将用户问题扩展为多个相关查询

### 2. 优化生成质量

- **多轮对话**：支持上下文记忆
- **置信度评估**：对低置信度回答进行标注
- **幻觉检测**：检测生成内容是否与检索文档一致

### 3. 高级RAG技术

- **Self-RAG**：自省式RAG，边回答边优化
- **GraphRAG**：基于知识图谱的RAG
- **T-RAG**：时序增强RAG，支持动态更新

## 🐛 常见问题

### Q1: 向量存储文件很大？

**A**: 这是正常的。每个文档块都会生成一个向量（1536维），存储所有向量需要一定空间。可以通过以下方式优化：
- 减少文档数量
- 使用更小的embedding模型
- 使用压缩技术

### Q2: 检索结果不准确？

**A**: 可以尝试：
- 调整分块大小和重叠
- 增加检索数量（k值）
- 优化查询问题（更具体、更明确）
- 使用更好的embedding模型

### Q3: 生成回答有幻觉？

**A**: 可以：
- 加强Prompt约束（明确要求基于文档回答）
- 添加置信度评估
- 使用RAGAS等工具评估

### Q4: 响应速度慢？

**A**: 可以：
- 使用更快的embedding模型
- 减少检索数量
- 使用GPU加速
- 缓存常见查询

## 📚 参考资料

- [LangChain官方文档](https://python.langchain.com/)
- [FAISS文档](https://github.com/facebookresearch/faiss)
- [OpenAI Embeddings文档](https://platform.openai.com/docs/guides/embeddings)
- [RAG论文](https://arxiv.org/abs/2005.11401)

## 🤝 贡献

欢迎提交Issue和Pull Request！

## 📄 许可证

MIT License

---

**祝你学习愉快！通过这个项目，你将全面掌握RAG技术的核心要点。** 🚀

