"""
å‘é‡å­˜å‚¨æ¨¡å—ï¼šè´Ÿè´£å°†æ–‡æ¡£å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“

æ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼š
1. Embeddingå‘é‡åŒ–ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡ï¼ˆè¯­ä¹‰çš„æ•°å­¦è¡¨è¾¾ï¼‰
2. å‘é‡æ•°æ®åº“ï¼šé«˜æ•ˆå­˜å‚¨å’Œæ£€ç´¢å‘é‡æ•°æ®
3. ç›¸ä¼¼åº¦è®¡ç®—ï¼šä½™å¼¦ç›¸ä¼¼åº¦ã€æ¬§æ°è·ç¦»ç­‰
"""
import os
from typing import List, Optional
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.vectorstores.base import VectorStore


class VectorStoreManager:
    """å‘é‡å­˜å‚¨ç®¡ç†å™¨ï¼šè´Ÿè´£æ–‡æ¡£å‘é‡åŒ–å’Œå‘é‡æ•°æ®åº“ç®¡ç†"""
    
    def __init__(self, embedding_model: str = "text-embedding-3-small"):
        """
        åˆå§‹åŒ–å‘é‡å­˜å‚¨ç®¡ç†å™¨
        
        Args:
            embedding_model: Embeddingæ¨¡å‹åç§°
        """
        # åˆå§‹åŒ–OpenAI Embeddingæ¨¡å‹
        # è¿™ä¸ªæ¨¡å‹ä¼šå°†æ–‡æœ¬è½¬æ¢ä¸º1536ç»´çš„å‘é‡
        self.embeddings = OpenAIEmbeddings(
            model=embedding_model,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        self.vector_store: Optional[VectorStore] = None
    
    def create_vector_store(self, documents: List[Document]) -> VectorStore:
        """
        åˆ›å»ºå‘é‡å­˜å‚¨å¹¶æ·»åŠ æ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            å‘é‡å­˜å‚¨å¯¹è±¡
        """
        print("ğŸ”„ å¼€å§‹å‘é‡åŒ–æ–‡æ¡£...")
        
        # ä½¿ç”¨FAISSåˆ›å»ºå‘é‡å­˜å‚¨
        # FAISSä¼šè‡ªåŠ¨è°ƒç”¨embeddingæ¨¡å‹å°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡
        # å¹¶å»ºç«‹ç´¢å¼•ä»¥ä¾¿å¿«é€Ÿæ£€ç´¢
        self.vector_store = FAISS.from_documents(
            documents=documents,
            embedding=self.embeddings
        )
        
        print(f"âœ… æˆåŠŸåˆ›å»ºå‘é‡å­˜å‚¨ï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£å—")
        return self.vector_store
    
    def save_vector_store(self, save_path: str):
        """
        ä¿å­˜å‘é‡å­˜å‚¨åˆ°ç£ç›˜
        
        Args:
            save_path: ä¿å­˜è·¯å¾„
        """
        if self.vector_store is None:
            raise ValueError("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåˆ›å»ºå‘é‡å­˜å‚¨")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        # ä¿å­˜å‘é‡å­˜å‚¨
        self.vector_store.save_local(save_path)
        print(f"âœ… å‘é‡å­˜å‚¨å·²ä¿å­˜åˆ°: {save_path}")
    
    def load_vector_store(self, load_path: str) -> VectorStore:
        """
        ä»ç£ç›˜åŠ è½½å‘é‡å­˜å‚¨
        
        Args:
            load_path: åŠ è½½è·¯å¾„
            
        Returns:
            å‘é‡å­˜å‚¨å¯¹è±¡
        """
        if not os.path.exists(load_path):
            raise FileNotFoundError(f"å‘é‡å­˜å‚¨ä¸å­˜åœ¨: {load_path}")
        
        # åŠ è½½å‘é‡å­˜å‚¨
        self.vector_store = FAISS.load_local(
            load_path=load_path,
            embeddings=self.embeddings,
            allow_dangerous_deserialization=True  # FAISSéœ€è¦æ­¤å‚æ•°
        )
        
        print(f"âœ… æˆåŠŸåŠ è½½å‘é‡å­˜å‚¨: {load_path}")
        return self.vector_store
    
    def similarity_search(self, query: str, k: int = 3) -> List[Document]:
        """
        ç›¸ä¼¼åº¦æœç´¢ï¼šæ ¹æ®æŸ¥è¯¢æ–‡æœ¬æ‰¾åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›æœ€ç›¸å…³çš„kä¸ªæ–‡æ¡£
            
        Returns:
            æœ€ç›¸å…³çš„æ–‡æ¡£åˆ—è¡¨
        """
        if self.vector_store is None:
            raise ValueError("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåˆ›å»ºæˆ–åŠ è½½å‘é‡å­˜å‚¨")
        
        # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢
        # FAISSä¼šï¼š
        # 1. å°†æŸ¥è¯¢æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
        # 2. è®¡ç®—æŸ¥è¯¢å‘é‡ä¸æ‰€æœ‰æ–‡æ¡£å‘é‡çš„ç›¸ä¼¼åº¦ï¼ˆé»˜è®¤ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        # 3. è¿”å›æœ€ç›¸ä¼¼çš„kä¸ªæ–‡æ¡£
        results = self.vector_store.similarity_search(query, k=k)
        
        return results
    
    def similarity_search_with_score(self, query: str, k: int = 3) -> List[tuple]:
        """
        ç›¸ä¼¼åº¦æœç´¢å¹¶è¿”å›ç›¸ä¼¼åº¦åˆ†æ•°
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›æœ€ç›¸å…³çš„kä¸ªæ–‡æ¡£
            
        Returns:
            (æ–‡æ¡£, ç›¸ä¼¼åº¦åˆ†æ•°) å…ƒç»„åˆ—è¡¨
        """
        if self.vector_store is None:
            raise ValueError("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåˆ›å»ºæˆ–åŠ è½½å‘é‡å­˜å‚¨")
        
        # è¿”å›æ–‡æ¡£å’Œç›¸ä¼¼åº¦åˆ†æ•°
        results = self.vector_store.similarity_search_with_score(query, k=k)
        
        return results


def demo_vector_store():
    """æ¼”ç¤ºå‘é‡å­˜å‚¨åŠŸèƒ½"""
    print("=" * 60)
    print("å‘é‡å­˜å‚¨æ¨¡å—æ¼”ç¤º")
    print("=" * 60)
    
    from document_loader import DocumentLoader
    
    # 1. åŠ è½½æ–‡æ¡£
    loader = DocumentLoader(chunk_size=500, chunk_overlap=75)
    data_dir = os.path.join(os.path.dirname(__file__), "data")
    file_path = os.path.join(data_dir, "hr_policy.txt")
    
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return
    
    documents = loader.load_and_split(file_path)
    
    # 2. åˆ›å»ºå‘é‡å­˜å‚¨
    vector_manager = VectorStoreManager()
    vector_store = vector_manager.create_vector_store(documents)
    
    # 3. æµ‹è¯•ç›¸ä¼¼åº¦æœç´¢
    test_queries = [
        "å¹´å‡å¦‚ä½•ç”³è¯·ï¼Ÿ",
        "äº§å‡æœ‰å¤šå°‘å¤©ï¼Ÿ",
        "å·¥èµ„ä»€ä¹ˆæ—¶å€™å‘æ”¾ï¼Ÿ"
    ]
    
    print("\n" + "=" * 60)
    print("ç›¸ä¼¼åº¦æœç´¢æµ‹è¯•")
    print("=" * 60)
    
    for query in test_queries:
        print(f"\nğŸ” æŸ¥è¯¢: {query}")
        results = vector_manager.similarity_search_with_score(query, k=2)
        
        for i, (doc, score) in enumerate(results, 1):
            print(f"\n  ç»“æœ {i} (ç›¸ä¼¼åº¦: {score:.4f}):")
            print(f"  {doc.page_content[:200]}...")
    
    # 4. ä¿å­˜å‘é‡å­˜å‚¨
    save_path = os.path.join(os.path.dirname(__file__), "storage", "vectorstore")
    vector_manager.save_vector_store(save_path)
    
    print("\nâœ… æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    demo_vector_store()

